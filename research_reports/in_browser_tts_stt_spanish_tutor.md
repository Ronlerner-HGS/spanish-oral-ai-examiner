# Practical implementation options for in-browser TTS and STT in a web Spanish tutor

## Scope and framing

A web Spanish tutor that “speaks” prompts and listens to learner responses is best thought of as two loosely coupled subsystems: text-to-speech playback for tutor output and speech capture plus transcription for learner input. In the browser, the highest-level path for both is the Web Speech API, but its real-world portability differs sharply between TTS and STT. The practical conclusion from current compatibility data is that you can usually rely on Web Speech for TTS on modern Chrome and Safari (including iOS), but you cannot rely on Web Speech for STT across browsers; you should design STT as a progressive enhancement and have a robust recording plus server transcription fallback.

## Web Speech API for text-to-speech (SpeechSynthesis)

The Speech Synthesis portion of the Web Speech API is broadly available in Chrome and Safari families. Compatibility tables show support in Chrome and in Safari desktop and iOS Safari across many versions, with older iOS Safari lacking it prior to iOS 7.[^https://caniuse.com/speech-synthesis] In practice, the main constraints for a tutor app are not “does the API exist” but “will audio be allowed to play now,” “are Spanish voices available,” and “do the events behave reliably enough to drive turn-taking.”

Voice availability is inherently platform dependent because browsers expose the operating system’s installed voices rather than a uniform, web-hosted set. MDN’s `SpeechSynthesisVoice` documentation emphasizes that voices are discovered at runtime and vary by user agent and device, which is why you must treat language and voice selection as best-effort rather than guaranteed.[^https://developer.mozilla.org/en-US/docs/Web/API/SpeechSynthesisVoice] In a Spanish tutor, this means that a learner on iOS may have only a small set of Spanish voices and may not have your preferred locale variant (for example, a Latin American voice versus castilian). It also means you should be prepared for the voice list to be empty briefly at startup due to asynchronous loading.

The `speechSynthesis.getVoices()` list is often populated asynchronously, and the intended pattern is to listen for the `voiceschanged` event and only then populate selection UI or pick a default voice.[^https://developer.mozilla.org/en-US/docs/Web/API/SpeechSynthesis/getVoices][^https://developer.mozilla.org/en-US/docs/Web/API/SpeechSynthesis/voiceschanged_event] For a one-day project, the simplest robust behavior is to pick by language (`es`, `es-ES`, `es-MX`) once voices are available and otherwise fall back to whatever the browser chooses when you set `utterance.lang`.

Autoplay and user-gesture constraints are the central UX gotcha. Browsers increasingly treat speech synthesis similarly to other audio playback: they may require prior user activation before a page can start audible output. Chrome’s developer discussions around deprecating and then disallowing `speechSynthesis.speak()` without user activation document that calling `speak()` may fail (“not-allowed”) if the frame has not had activation, aligning it with autoplay policy.[^https://groups.google.com/a/chromium.org/g/blink-dev/c/WsnBm53M4Pc] MDN’s general autoplay guide explains that when autoplay is blocked, `play()`-like operations are rejected with `NotAllowedError`, and user interaction is typically required to allow playback.[^https://developer.mozilla.org/en-US/docs/Web/Media/Autoplay_guide] On iOS Safari, similar user-gesture constraints exist for audible playback as part of WebKit’s media policies; WebKit’s own discussion of iOS video policies provides the concrete definition of a “user gesture” for media playback as an action directly resulting from trusted events such as `touchend`, `click`, or `keydown`.[^https://webkit.org/blog/6784/new-video-policies-for-ios]

For turn-based tutoring, you should avoid trying to “auto-speak” on page load. Instead, design the first tutor utterance to be triggered by an explicit “Start lesson” button. Once the page has user activation, subsequent prompts are usually permitted during that page session, but you should still provide a visible “Tap to hear” recovery affordance if speech fails.

A second practical gotcha is timing and event reliability. The tutorial-style guidance from Google’s introduction to the Speech Synthesis API notes that speech can be queued and that you should manage utterance lifecycle events; in real use, developers encounter occasional missing `end` events in some Chromium scenarios, so your “move to next question when speaking ends” logic should include a timeout backstop rather than relying on events alone.[^https://developers.google.com/web/updates/2014/01/Web-apps-that-talk-Introduction-to-the-Speech-Synthesis-API]

## Web Speech API for speech-to-text (SpeechRecognition)

Speech recognition via `SpeechRecognition` is not a portable baseline feature. MDN explicitly labels SpeechRecognition as “limited availability” and “not Baseline,” meaning you should not assume it works in widely used browsers.[^https://developer.mozilla.org/en-US/docs/Web/API/SpeechRecognition] Cross-browser compatibility tables show that the Speech Recognition API is supported in Chrome with historic reliance on the `webkitSpeechRecognition` prefix, while Safari desktop and iOS Safari are marked as only partially supported from Safari 14.1 (macOS) and iOS 14.5 onward, and many other browsers either lack support or have incomplete pieces.[^https://caniuse.com/speech-recognition] This “partial” marking tracks with common field reports of instability, device settings dependencies, and inconsistent event delivery.

Where recognition exists, you still face two classes of failures that matter for a tutor. The first is permission and secure-context gating. Microphone access depends on `getUserMedia`, which is only available in secure contexts such as HTTPS (or localhost/file), and attempts in insecure contexts fail.[^https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getUserMedia] The second is “service not allowed” and “not allowed” errors that reflect either denial of permission or browser refusal to use a recognition service. MDN’s documentation of SpeechRecognition error values distinguishes `not-allowed` from `service-not-allowed`, where the latter can occur when the user agent disallows the requested recognition service and might prefer another service.[^https://developer.mozilla.org/en-US/docs/Web/API/SpeechRecognitionErrorEvent/error] In practice, you should treat either as a signal to fall back to recorded-audio transcription.

Because SpeechRecognition’s behavior differs between platforms and may depend on OS features (for example, Siri/dictation settings on Apple devices), a one-day project should not depend on it for core functionality. The most pragmatic use is as an opportunistic fast-path on desktop Chrome where it is typically the smoothest, while designing the rest of the system around the fallback.

## Fallback: in-browser recording with MediaRecorder and server-side transcription (Whisper family, Groq)

A robust, browser-agnostic STT approach is to record audio locally and send it to the server for transcription. The browser piece is capturing microphone audio and encoding it to a form you can upload, and the server piece is calling an ASR model endpoint.

On the client side, `MediaRecorder` has strong support in Chromium browsers and is supported in Safari desktop and iOS Safari from the mid-iOS-14 era onward according to compatibility tables.[^https://caniuse.com/mediarecorder] This makes it a realistic cross-platform primitive for modern users, but you should still plan for older iOS devices where MediaRecorder is absent. For those, the remaining pragmatic fallback is a file input with `accept="audio/*" capture`, which uses the OS recorder UI rather than in-page recording.

The recording flow still requires an HTTPS origin because it depends on microphone capture via `getUserMedia`.[^https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getUserMedia] It also inherits the same permission UX: the first time a user records, they will see a microphone prompt. A tutor should incorporate a short “We need mic access to hear you” explainer before triggering the prompt to reduce denial rates, and should have a clear path to retry if permission is denied.

On the server side, Groq documents an OpenAI-compatible Speech-to-Text endpoint at `/openai/v1/audio/transcriptions` and recommends the `whisper-large-v3-turbo` model for fast multilingual transcription.[^https://console.groq.com/docs/speech-to-text][^https://console.groq.com/docs/model/whisper-large-v3-turbo] Groq’s model guidance also emphasizes that Whisper-family models are optimized around roughly 30-second segments.[^https://console.groq.com/docs/model/whisper-large-v3-turbo] For a tutor experience, that aligns well with “record answer, stop, transcribe,” but it discourages very long continuous recordings; you should structure learner input into short utterances, with explicit stop controls, and enforce an upper bound.

A practical design choice is whether to upload the entire recording after stop, or to chunk and upload progressively. Chunking can reduce perceived latency for longer answers but adds complexity. For a one-day project, uploading after stop with a 5–15 second soft target is typically sufficient.

## Recommended approach for a one-day project

A one-day prototype should prioritize reliability over sophistication and should treat voice I/O as optional enhancements rather than core navigation. The recommended architecture is to implement TTS with `SpeechSynthesis` as the default tutor voice where supported and to implement STT primarily via MediaRecorder plus server transcription, using Web Speech recognition only as a best-effort fast path when present.

Concretely, the tutor can have a “Start” interaction that both unlocks audio playback for TTS and sets user expectations about mic permission. Immediately after that, you can pre-load the voice list (listening for `voiceschanged`) and pick a Spanish voice if present.[^https://developer.mozilla.org/en-US/docs/Web/API/SpeechSynthesis/voiceschanged_event] For STT, you can show a single large “Hold to talk” or “Tap to record” control; when pressed, call `getUserMedia` and record with MediaRecorder if available. If `SpeechRecognition` is present and you are on a platform where you trust it (often desktop Chrome), you can offer a toggle “live dictation” mode, but the default should remain the record-and-transcribe path.

The key reason this hybrid is appropriate for a one-day build is that SpeechSynthesis tends to work across your target browsers while SpeechRecognition does not, and the MediaRecorder plus server transcription path gives you a consistent core that behaves similarly on Chrome and Safari families based on standardized media capture plus a single ASR model.

## Key UX gotchas and how to mitigate them

Permissions are the first failure point. Microphone capture requires HTTPS and explicit user consent; the permission prompt is disruptive and denial is common if the app triggers it without context. Because `getUserMedia` is secure-context gated, a prototype should be deployed on HTTPS (even for testing) to avoid premature failure.[^https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getUserMedia] When permission is denied, do not dead-end; instead, provide instructions to re-enable mic permissions and offer a manual text entry fallback so the lesson can continue.

Autoplay restrictions are the second failure point and affect both TTS and any attempt to play recorded audio back. Design all audio output to be initiated from user interactions, at least for the first utterance, consistent with browser autoplay policies.[^https://developer.mozilla.org/en-US/docs/Web/Media/Autoplay_guide] WebKit’s definition of acceptable gestures underscores why you should use a direct tap/click on a control rather than indirect triggers such as page load or timers for the initial speak.[^https://webkit.org/blog/6784/new-video-policies-for-ios] In practice, a “Start lesson” button that plays a short Spanish greeting is an effective way to both unlock audio and confirm that sound works.

Timing feedback matters because both TTS voice loading and server transcription introduce latency. Speech synthesis voices may not be immediately available at page load, which is why the `voiceschanged` event should drive voice selection and why you should handle the empty-voice-list case gracefully.[^https://developer.mozilla.org/en-US/docs/Web/API/SpeechSynthesis/getVoices] Server transcription will feel slow if the UI gives no feedback, so you should show the recording state distinctly, then show an explicit “Transcribing…” state, then display recognized text with an easy “Try again” action.

Turn-taking is a subtle but important UX problem. If you speak a prompt and then immediately start recording, you risk capturing the tutor audio (“echo”) and transcribing it as the learner’s response. The simplest mitigation in a one-day project is to enforce a strict state machine: never record while TTS is speaking, and add a small post-speak delay (for example, a few hundred milliseconds) before enabling recording. When events are unreliable, you should still have a timeout-based recovery path so the interface does not get stuck waiting for a speaking-end event that never arrives.

Finally, language configuration impacts accuracy. For SpeechRecognition, you should set `lang` (e.g., `es-ES`) when using it, but because SpeechRecognition is unreliable across browsers, the more consistent lever is server-side ASR. Groq’s transcription endpoint supports language hints, and providing `language=es` (or the appropriate locale) reduces wrong-language decoding and improves recognition on short utterances.[^https://console.groq.com/docs/speech-to-text]

## Incremental hardening beyond the prototype

After the one-day prototype, the natural hardening steps are to add capability detection and routing, add resilience around state transitions, and reduce latency. Capability detection should decide among three STT tiers: Web Speech recognition where it behaves well, MediaRecorder plus server transcription where supported, and OS-level capture via file input where MediaRecorder is missing. Resilience means explicit surfacing of mic and audio errors using the documented error modes of SpeechRecognition (`not-allowed`, `service-not-allowed`) and the promise rejection patterns described in autoplay policy guidance.[^https://developer.mozilla.org/en-US/docs/Web/API/SpeechRecognitionErrorEvent/error][^https://developer.mozilla.org/en-US/docs/Web/Media/Autoplay_guide] Latency improvements can come from shorter recordings, optional chunking for longer responses, and careful UI pacing rather than attempting continuous dictation.

## High-level abstract

For a web Spanish tutor, SpeechSynthesis (Web Speech API) is the most practical in-browser TTS option across Chrome and Safari including iOS, but it must be initiated from a user gesture and requires asynchronous voice loading handling via `voiceschanged`. SpeechRecognition (Web Speech API) remains too inconsistent across browsers, with only partial Safari support and common permission/service errors, so STT should be built around recording with MediaRecorder (supported in modern Safari iOS from around 14.5+) plus server transcription using a Whisper-family model such as Groq’s OpenAI-compatible `/openai/v1/audio/transcriptions` endpoint. A one-day project should therefore ship a hybrid: TTS via SpeechSynthesis and STT via MediaRecorder→server transcription as the default, with SpeechRecognition as an optional fast-path on platforms where it’s known to work better.
